{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newspaper3k\n",
    "!pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-28T23:06:30.544958Z",
     "iopub.status.busy": "2025-01-28T23:06:30.544665Z",
     "iopub.status.idle": "2025-01-28T23:52:15.980123Z",
     "shell.execute_reply": "2025-01-28T23:52:15.979217Z",
     "shell.execute_reply.started": "2025-01-28T23:06:30.544936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 articles for category: culture\n",
      "Collected 1000 articles for category: sport\n",
      "Collected 1000 articles for category: technology\n",
      "Collected 1000 articles for category: science\n",
      "Collected 0 articles for category: health\n",
      "Collected 1000 articles for category: world\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import requests\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def preprocess_and_vectorize(text, model_name='bert-base-uncased', max_length=512):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text and converts it into a vector using BERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text/document.\n",
    "        model_name (str): The name of the pre-trained BERT model to use.\n",
    "        max_length (int): Maximum length of the tokenized input.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A vector representation of the input text.\n",
    "    \"\"\"\n",
    "    # Load pre-trained BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings\n",
    "    # Use the [CLS] token embedding as the document representation\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "    return cls_embedding.cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "\n",
    "\n",
    "# API and file setup\n",
    "base_url = \"https://content.guardianapis.com/search\"\n",
    "api_key = \"test\"  # Public API key (no authentication required)\n",
    "vector_file_path = '\\data\\preprocessed\\documents_vectors.json'\n",
    "categories = [\"culture\", \"sport\", \"technology\", \"science\", \"health\", \"world\"]\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(vector_file_path, 'a') as vector_file:\n",
    "    for category in categories:\n",
    "        page = 1\n",
    "        total_articles_collected = 0\n",
    "\n",
    "        while total_articles_collected < 1000:\n",
    "            params = {\n",
    "                \"section\": category,\n",
    "                \"page-size\": 50,  # Maximum allowed per request\n",
    "                \"page\": page,\n",
    "                \"api-key\": api_key,\n",
    "                \"show-fields\": \"body\"  # Include full article content\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch articles for {category}: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            articles = response.json().get(\"response\", {}).get(\"results\", [])\n",
    "\n",
    "            if not articles:\n",
    "                break  # No more articles available\n",
    "\n",
    "            for i, article in enumerate(articles):\n",
    "                url = article.get(\"webUrl\")\n",
    "                try:\n",
    "                    # Extract the article content from the API response\n",
    "                    content = article.get(\"fields\", {}).get(\"body\", \"\")\n",
    "\n",
    "                    if not content:\n",
    "                        # Fallback to newspaper3k if content is not available\n",
    "                        news_article = Article(url)\n",
    "                        news_article.download()\n",
    "                        news_article.parse()\n",
    "                        content = news_article.text\n",
    "\n",
    "                    # Get the vector representation of the article\n",
    "                    document_vector = preprocess_and_vectorize(content)\n",
    "\n",
    "                    # Save the vector and metadata to the JSON file\n",
    "                    vector_entry = {\n",
    "                        'id': total_articles_collected + 1,\n",
    "                        'vector': document_vector,\n",
    "                        'category': category,\n",
    "                        'url': url\n",
    "                    }\n",
    "                    json.dump(vector_entry, vector_file)\n",
    "                    vector_file.write('\\n')  # Add newline for each entry\n",
    "\n",
    "                    total_articles_collected += 1\n",
    "\n",
    "                    if total_articles_collected >= 1000:\n",
    "                        break  # Stop after collecting 1000 articles\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article {url}: {e}\")\n",
    "\n",
    "            page += 1  # Move to the next page\n",
    "\n",
    "        print(f\"Collected {total_articles_collected} articles for category: {category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T23:52:36.588470Z",
     "iopub.status.busy": "2025-01-28T23:52:36.588172Z",
     "iopub.status.idle": "2025-01-28T23:52:59.452796Z",
     "shell.execute_reply": "2025-01-28T23:52:59.452080Z",
     "shell.execute_reply.started": "2025-01-28T23:52:36.588447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'culture': 1000, 'sport': 1000, 'technology': 1000, 'science': 1000, 'world': 1000})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.6108 - loss: 1.1071 - val_accuracy: 0.8920 - val_loss: 0.3279\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8906 - loss: 0.3318 - val_accuracy: 0.8920 - val_loss: 0.3429\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8976 - loss: 0.2848 - val_accuracy: 0.9180 - val_loss: 0.2513\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9278 - loss: 0.2144 - val_accuracy: 0.9160 - val_loss: 0.2608\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9281 - loss: 0.2015 - val_accuracy: 0.9120 - val_loss: 0.2555\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9375 - loss: 0.1953 - val_accuracy: 0.9100 - val_loss: 0.2807\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9268 - loss: 0.2123 - val_accuracy: 0.9280 - val_loss: 0.2392\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9460 - loss: 0.1613 - val_accuracy: 0.9300 - val_loss: 0.2329\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9572 - loss: 0.1405 - val_accuracy: 0.9300 - val_loss: 0.2537\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9569 - loss: 0.1265 - val_accuracy: 0.9240 - val_loss: 0.2455\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.1166\n",
      "Test Accuracy: 0.9596\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "def filter_categories_with_insufficient_samples(vectors, categories, min_samples=4):\n",
    "    \"\"\"\n",
    "    Filters out categories with fewer than `min_samples` samples.\n",
    "\n",
    "    Args:\n",
    "        vectors (np.array): Array of document vectors.\n",
    "        categories (np.array): Array of corresponding categories.\n",
    "        min_samples (int): Minimum number of samples required for each category.\n",
    "\n",
    "    Returns:\n",
    "        filtered_vectors (np.array): Filtered array of document vectors.\n",
    "        filtered_categories (np.array): Filtered array of corresponding categories.\n",
    "    \"\"\"\n",
    "    # Count the number of samples per category\n",
    "    category_counts = Counter(categories)\n",
    "\n",
    "    # Identify categories with sufficient samples\n",
    "    valid_categories = [category for category, count in category_counts.items() if count >= min_samples]\n",
    "\n",
    "    # Filter vectors and categories\n",
    "    mask = np.isin(categories, valid_categories)\n",
    "    filtered_vectors = vectors[mask]\n",
    "    filtered_categories = categories[mask]\n",
    "\n",
    "    return filtered_vectors, filtered_categories\n",
    "\n",
    "def load_data_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads data (vectors and categories) from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        vectors (list): List of document vectors.\n",
    "        categories (list): List of corresponding categories.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    categories = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            vectors.append(entry['vector'])\n",
    "            categories.append(entry['category'])\n",
    "\n",
    "    return np.array(vectors), np.array(categories)\n",
    "\n",
    "\n",
    "def train_lstm_model(vectors, categories, num_classes=9, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model to predict the category of a document.\n",
    "\n",
    "    Args:\n",
    "        vectors (np.array): Array of document vectors.\n",
    "        categories (np.array): Array of corresponding categories.\n",
    "        num_classes (int): Number of unique categories.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained LSTM model.\n",
    "        history: Training history.\n",
    "    \"\"\"\n",
    "    # Filter out categories with insufficient samples\n",
    "    vectors, categories = filter_categories_with_insufficient_samples(vectors, categories)\n",
    "\n",
    "    # Encode categories into integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    categories_encoded = label_encoder.fit_transform(categories)\n",
    "    categories_one_hot = to_categorical(categories_encoded, num_classes=num_classes)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        vectors, categories_one_hot, test_size=test_size, stratify=categories, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Reshape input data for LSTM (samples, timesteps, features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Use MirroredStrategy to distribute the model across 2 GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data from JSON file\n",
    "    file_path = '/kaggle/working/documents_vectors.json'\n",
    "    vectors, categories = load_data_from_json(file_path)\n",
    "    \n",
    "    print(Counter(categories))\n",
    "    # Train the LSTM model\n",
    "    model, history = train_lstm_model(vectors, categories)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(\n",
    "        vectors.reshape((vectors.shape[0], 1, vectors.shape[1])),\n",
    "        to_categorical(LabelEncoder().fit_transform(categories), num_classes=9)\n",
    "    )\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
